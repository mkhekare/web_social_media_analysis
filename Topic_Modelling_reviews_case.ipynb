{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyThf8Ve4Bzo"
      },
      "outputs": [],
      "source": [
        "!pip install pandas matplotlib wordcloud nltk gensim pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIgC-e6m3843"
      },
      "outputs": [],
      "source": [
        "# 0 Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "from google.colab import files\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Install nltk stopwords and import additional libraries\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2c6B41D388b"
      },
      "outputs": [],
      "source": [
        "#1 Upload file with reviews\n",
        "file_name = input(\"Enter filename with reviews. it should one column review\")\n",
        "# file_name = 'Reviews_Mobile_01.csv'\n",
        "\n",
        "df = pd.read_csv(file_name)\n",
        "df.dropna(subset=['review'], inplace=True)\n",
        "print(\"Number of Rows:\")\n",
        "print( df.shape[0])\n",
        "print(\"SAMPLE ROWS:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHRfkOQd4U6C"
      },
      "outputs": [],
      "source": [
        "# 2. Convert to lowercase\n",
        "df['cleaned_text'] = df['review'].str.lower()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix_tucua4zgH"
      },
      "outputs": [],
      "source": [
        "# 3. Remove punctuation\n",
        "df['cleaned_text'] = df['cleaned_text'].str.replace('[^\\w\\s]', '', regex=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skJO7S-k4zkS"
      },
      "outputs": [],
      "source": [
        "# 4. Remove stop words\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA3kk6tb4ztB"
      },
      "outputs": [],
      "source": [
        "# 5. Generate Word Cloud\n",
        "text = ' '.join(df['cleaned_text'])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Save word cloud as an image\n",
        "wordcloud.to_file(\"wordcloud.png\")\n",
        "# files.download(\"wordcloud.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Think is the number of samples sufficient to do Topic Mondelling?- can we just make sense from manual inspection for this case"
      ],
      "metadata": {
        "id": "44X4OztCoxS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine the optimal number of topics\n",
        "\n",
        "def determine_optimal_topics(corpus, dictionary, texts, max_topics=10):\n",
        "    coherence_scores = []\n",
        "    for num_topics in range(2, max_topics + 1):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100)\n",
        "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_scores.append(coherence_model.get_coherence())\n",
        "    optimal_topics = coherence_scores.index(max(coherence_scores)) + 2\n",
        "    return optimal_topics\n",
        "\n",
        "tokenized_text = df['cleaned_text'].apply(lambda x: x.split())  # Tokenization\n",
        "dictionary = Dictionary(tokenized_text)\n",
        "corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
        "\n",
        "# Ask user for the number of topics\n",
        "user_choice = input(\"Enter 'auto' to let the code determine the number of topics, or specify a number (2-10): \")\n",
        "\n",
        "if user_choice.lower() == 'auto':\n",
        "    num_topics = determine_optimal_topics(corpus, dictionary, tokenized_text)  # Ensure you have tokenized_text defined\n",
        "    print(f\"Optimal number of topics determined: {num_topics}\")\n",
        "else:\n",
        "    num_topics = int(user_choice)\n",
        "\n",
        "# Topic Modeling and Visualization\n",
        "def topic_modeling(corpus, dictionary, num_topics, df):\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100)\n",
        "\n",
        "    # Generate pyLDAvis visualization and save as HTML\n",
        "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "    pyLDAvis.save_html(vis, 'lda_topic_visualization.html')\n",
        "    print(\"LDA visualization saved as 'lda_topic_visualization.html'\")\n",
        "\n",
        "    # Topic assignments and probabilities\n",
        "    topic_assignments = []\n",
        "    topic_probabilities_list = []\n",
        "\n",
        "    for doc_bow in corpus:\n",
        "        topic_probabilities = lda_model.get_document_topics(doc_bow, minimum_probability=0)\n",
        "        probabilities = [prob for _, prob in topic_probabilities]\n",
        "        topic_probabilities_list.append(probabilities)  # Store all topic probabilities\n",
        "\n",
        "        if topic_probabilities:\n",
        "            max_prob, max_topic = max(topic_probabilities, key=lambda x: x[1])\n",
        "            if max_prob >= 0.6:\n",
        "                topic_assignments.append(max_topic)\n",
        "            else:\n",
        "                topic_assignments.append(-1)  # Assign -1 for \"None\" topic\n",
        "        else:\n",
        "            topic_assignments.append(-1)  # Assign -1 for \"None\" topic\n",
        "\n",
        "    # Create a DataFrame for the topic probabilities\n",
        "    probabilities_df = pd.DataFrame(topic_probabilities_list, columns=[f'prob_topic_{i}' for i in range(num_topics)])\n",
        "\n",
        "    # Add topic assignments and probabilities to original data\n",
        "    df['topic_assignment'] = topic_assignments\n",
        "    df = pd.concat([df, probabilities_df], axis=1)  # Concatenate the probabilities DataFrame\n",
        "    df['max_topic'] = probabilities_df.idxmax(axis=1).apply(lambda x: x.replace('prob_topic_', ''))  # Add max topic name column\n",
        "\n",
        "    return lda_model, topic_assignments, topic_probabilities_list, vis, df\n",
        "\n",
        "# Run the topic modeling function, passing df as an argument\n",
        "lda_model, topic_assignments, topic_probabilities_list, vis, df = topic_modeling(corpus, dictionary, num_topics, df)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('reviews_with_topics.csv', index=False)\n",
        "\n",
        "# Save topic-word mapping\n",
        "topic_words = {i: [word for word, _ in lda_model.show_topic(i)] for i in range(num_topics)}\n",
        "topic_words_df = pd.DataFrame.from_dict(topic_words, orient='index').transpose()\n",
        "topic_words_df.to_csv('topic_words_mapping.csv', index=False)\n",
        "\n",
        "# Save frequency distribution of topic assignments including \"None\" topic\n",
        "topic_frequency = df['topic_assignment'].value_counts().sort_index()\n",
        "topic_frequency_table = pd.DataFrame({'Topic': topic_frequency.index, 'Review_Count': topic_frequency.values})\n",
        "topic_frequency_table.to_excel('topic_frequency_distribution.xlsx', index=False)\n",
        "print(\"Topic frequency distribution saved to 'topic_frequency_distribution.xlsx'.\")\n",
        "\n",
        "print(\"Processing complete. The topics have been assigned to reviews with high-probability thresholds.\")\n"
      ],
      "metadata": {
        "id": "gHet1mZbqcRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p1p8WsZHfDQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NgQjGqWwfDUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vSM0S06fDcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O-SaTriNNcAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}