{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step0: Import necessary libraries\n",
        "!pip install networkx matplotlib\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_N5_BkU-o-pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step1: Imp Retweet Network - Directed Weighted Graph\n",
        "# Expects a file with Vertex1 Vertex2 and weight\n",
        "# Load the uploaded file into a DataFrame\n",
        "filename  ='Imp_Retweet Network.xls'\n",
        "df = pd.read_excel(filename)\n",
        "df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cMXWzqI_wNv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step2 : Create an directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    G.add_edge(row['Vertex1'], row['Vertex2'], weight=row['Weight'])\n",
        "    # G.add_edge(row['mask_roll'], row['To'], weight=1)\n",
        "# Step 6: Draw the initial graph\n",
        "plt.figure(figsize=(8, 8))\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=False, node_size=500, node_color=\"lightblue\", font_size=10)\n",
        "plt.title(\"Undirected Graph\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xcb5-6zzT4fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Centrality calculations (Indegree)\n",
        "top_n = 20\n",
        "# In-Degree Centrality\n",
        "degree_centrality = nx.in_degree_centrality(G)\n",
        "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "node_colors = ['red' if node in dict(top_degree) else 'lightblue' for node in G.nodes()]\n",
        "nx.draw(G, pos, with_labels=False, node_color=node_colors, node_size=500, font_size=10)\n",
        "plt.title(\"Top Nodes by Degree In Centrality\")\n",
        "plt.show()\n",
        "print(\"Top nodes by Degree Centrality:\", top_degree)\n",
        "\n",
        "# Iterate through the list of top_degree using range\n",
        "for i in range(len(top_degree)):  # Use range to create an iterable sequence\n",
        "    print(top_degree[i][0], top_degree[i][1])\n",
        "\n"
      ],
      "metadata": {
        "id": "kUaNkKDjUL5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step4 Create a output file for the top degree centrality results\n",
        "top_degree_df = pd.DataFrame(top_degree, columns=['Tweet_id', 'In-Degree Centrality'])\n",
        "\n",
        "# Calculate and add the 'Degree' column to the DataFrame\n",
        "top_degree_df['Degree'] = top_degree_df['Tweet_id'].apply(lambda node: G.in_degree(node))\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "csv_filename = \"top_tweets_centrality.csv\"\n",
        "top_degree_df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Top tweets exported to {csv_filename}\")"
      ],
      "metadata": {
        "id": "xki7gVg_Qr_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step opt1 at the end is a programmatic way to export the tweets for these tweet ids. it can alos be done manually hanec, this step has been made optional"
      ],
      "metadata": {
        "id": "Kiq13QtCfmAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step5: Imp USER Network - Directed Weighted Graph\n",
        "# Expects a file with Vertex1 Vertex2 and weight\n",
        "# Load the uploaded file into a DataFrame\n",
        "filename  ='Imp_User_Network.xls'\n",
        "df = pd.read_excel(filename)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yO0Hko0RUMAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Community detection\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "communities = list(greedy_modularity_communities(G))\n",
        "\n",
        "# Assign colors to communities\n",
        "community_colors = {node: i for i, community in enumerate(communities) for node in community}\n",
        "node_colors = [community_colors[node] for node in G.nodes()]\n",
        "\n",
        "# Plot the graph with community colors\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(G, pos, with_labels=False, node_color=node_colors, cmap=plt.cm.rainbow, node_size=500, font_size=10)\n",
        "plt.title(\"Community Detection\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6L_kS4A3UMLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step7 : Create an directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    G.add_edge(row['Vertex1'], row['Vertex2'], weight=row['Weight'])\n",
        "    # G.add_edge(row['mask_roll'], row['To'], weight=1)\n",
        "# Step 6: Draw the initial graph\n",
        "plt.figure(figsize=(8, 8))\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=False, node_size=500, node_color=\"lightblue\", font_size=10)\n",
        "plt.title(\"Undirected Graph\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nKxQE7C-XoPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Centrality calculations (Outdegree) for users\n",
        "top_n = 20\n",
        "# Out-Degree Centrality\n",
        "degree_centrality = nx.out_degree_centrality(G)\n",
        "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "node_colors = ['red' if node in dict(top_degree) else 'lightblue' for node in G.nodes()]\n",
        "nx.draw(G, pos, with_labels=False, node_color=node_colors, node_size=500, font_size=10)\n",
        "plt.title(\"Top Nodes by Degree out Centrality\")\n",
        "plt.show()\n",
        "print(\"Top nodes by Degree Centrality:\", top_degree)\n",
        "\n",
        "# Iterate through the list of top_degree using range\n",
        "for i in range(len(top_degree)):  # Use range to create an iterable sequence\n",
        "    print(top_degree[i][0])\n"
      ],
      "metadata": {
        "id": "iMcHv-16rdSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# step9: Create a DataFrame for the top degree centrality users\n",
        "top_degree_df = pd.DataFrame(top_degree, columns=['Tweet_id', 'Out-Degree Centrality'])\n",
        "\n",
        "# Calculate and add the 'Degree' column to the DataFrame\n",
        "top_degree_df['Degree'] = top_degree_df['Tweet_id'].apply(lambda node: G.out_degree(node))\n",
        "\n",
        "# Export the DataFrame to a CSV file\n",
        "csv_filename = \"top_user_centrality.csv\"\n",
        "top_degree_df.to_csv(csv_filename, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"Top users exported to {csv_filename}\")"
      ],
      "metadata": {
        "id": "wlio36yn5WUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rG2p2nvi5WYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s63hF7RrlvYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5OLIYtBMlvbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_9__EwjlvfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HlbzAUu0lvkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQWJeqrw5Wb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ye1kYLP45WfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step Opt1 a :- Optional way to find the tweets belonging to highest centrality tweets\n",
        "# Do Topic Modelling on the tweets to find out the themes\n",
        "# StepA- Load the TWitter Data\n",
        "df_tweet = pd.read_excel(\"xoxoday_Raw_tweets.xls\", dtype = {\"In-Reply Tweet ID\": 'str', 'Imported ID': 'str'})\n",
        "df_tweet.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QxlsQnC75Wh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step Opt1 b - Load to get the tweets of relevance\n",
        "import pandas as pd\n",
        "\n",
        "# Load the top tweets centrality data\n",
        "top_tweets_df = pd.read_csv(\"top_tweets_centrality.csv\",  dtype = { \"Tweet_id\": 'str'})\n",
        "\n",
        "# Rename the column in top_tweets_df to match the column in df_tweet for joining\n",
        "# Check if \"Tweet\" is the actual column name in top_tweets_df that you want to rename\n",
        "top_tweets_df.rename(columns={\"Tweet_id\": \"Imported ID\"}, inplace = True)\n",
        "\n",
        "# Print columns of both DataFrames to verify names before merging\n",
        "print(\"Columns in top_tweets_df:\", top_tweets_df.columns)\n",
        "print(\"Columns in df_tweet:\", df_tweet.columns)\n",
        "\n",
        "# Perform the join using the correct column name\n",
        "# Ensure \"In-Reply Tweet ID\" is the correct column name in both DataFrames\n",
        "merged_df = pd.merge(top_tweets_df, df_tweet[[\"Imported ID\", \"Tweet\", 'Vertex 1', 'Relationship', \"Tweet Date (UTC)\"]], on=\"Imported ID\", how=\"inner\")\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(merged_df)\n",
        "\n",
        "# Export the merged DataFrame to a CSV file\n",
        "merged_df.to_csv(\"top_tweets.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Merged DataFrame exported to top_tweets.csv\")"
      ],
      "metadata": {
        "id": "wSmRuIc35Wki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step Opt2 b - Load to get the tweets of relevance\n",
        "import pandas as pd\n",
        "\n",
        "# Load the top tweets centrality data\n",
        "top_tweets_df = pd.read_csv(\"top_tweets_centrality.csv\",  dtype = { \"Tweet_id\": 'str'})\n",
        "\n",
        "# Rename the column in top_tweets_df to match the column in df_tweet for joining\n",
        "# Check if \"Tweet\" is the actual column name in top_tweets_df that you want to rename\n",
        "top_tweets_df.rename(columns={\"Tweet_id\": \"Imported ID\"}, inplace = True)\n",
        "\n",
        "# Print columns of both DataFrames to verify names before merging\n",
        "print(\"Columns in top_tweets_df:\", top_tweets_df.columns)\n",
        "print(\"Columns in df_tweet:\", df_tweet.columns)\n",
        "\n",
        "# Perform the join using the correct column name\n",
        "# Ensure \"In-Reply Tweet ID\" is the correct column name in both DataFrames\n",
        "merged_df = pd.merge(top_tweets_df, df_tweet[[\"Imported ID\", \"Tweet\", 'Vertex 1', 'Relationship', \"Tweet Date (UTC)\"]], on=\"Imported ID\", how=\"inner\")\n",
        "\n",
        "# Display the merged DataFrame\n",
        "print(merged_df)\n",
        "\n",
        "# Export the merged DataFrame to a CSV file\n",
        "merged_df.to_csv(\"top_tweets.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Merged DataFrame exported to top_tweets.csv\")"
      ],
      "metadata": {
        "id": "GUa64TG55Wnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bj0NtRzg5XkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sxrQLIOo5Xn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvwWw1wi5Xrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ft7sMWAa5Xup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x7jHNAaS5Xx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Vj8buxa5X06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSZvberMrdXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BjoLrPgXrPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UHjcgNWvrPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbIOToBnrPw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kta6qugfrP0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyJnRbRjrP3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ALqkHQsIrP62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}